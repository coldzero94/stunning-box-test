#!/usr/bin/env python3
import os
import argparse
import sys
from vllm.entrypoints.openai.api_server import main as vllm_api_server_main

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512,expandable_segments:True"

def main():
    """
    VLLMì˜ ë‚´ì¥ OpenAI í˜¸í™˜ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    íŒŒì¸íŠœë‹ëœ Qwen ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    base_model_name = "Qwen/Qwen2.5-14B-Instruct"
    adapter_path = "/qwen25-14b"  # íŒŒì¸íŠœë‹ëœ ì–´ëŒ‘í„° ê²½ë¡œ
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="VLLM OpenAI í˜¸í™˜ API ì„œë²„")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="ì„œë²„ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8000, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--model", type=str, default=base_model_name, help="ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--peft-model-path", type=str, default=adapter_path, help="PEFT/LoRA ì–´ëŒ‘í„° ê²½ë¡œ")
    args, unknown = parser.parse_known_args()
    
    print(f"ğŸ”§ ê¸°ë³¸ ëª¨ë¸: {args.model}")
    print(f"ğŸ”„ íŒŒì¸íŠœë‹ ì–´ëŒ‘í„°: {args.peft_model_path}")
    print(f"ğŸš€ ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì¤‘... http://{args.host}:{args.port}")
    
    # VLLM OpenAI í˜¸í™˜ ì„œë²„ ì‹¤í–‰ ì¸ì êµ¬ì„±
    vllm_args = [
        "--model", args.model,
        "--host", args.host,
        "--port", str(args.port),
        "--peft-model-path", args.peft_model_path,
        "--dtype", "half",  # float16ë¡œ ë³€í™˜
        "--max-model-len", "4096",
        "--trust-remote-code",  # ì›ê²© ì½”ë“œ ì‹ ë¢° (Qwen ëª¨ë¸ì— í•„ìš”)
        "--max-num-seqs", "256",  # ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
        "--gpu-memory-utilization", "0.9"
    ]
    
    # VLLM ë‚´ì¥ OpenAI API ì„œë²„ ì‹¤í–‰
    print("âœ¨ VLLM OpenAI í˜¸í™˜ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ’¡ API í˜¸ì¶œ ì˜ˆì‹œ: curl http://{args.host}:{args.port}/v1/chat/completions -H 'Content-Type: application/json' -d '{{\n  \"model\": \"{args.model}\",\n  \"messages\": [{{\n    \"role\": \"user\",\n    \"content\": \"ì•ˆë…•í•˜ì„¸ìš”\"\n  }}],\n  \"stream\": true\n}}'")
    
    # VLLM API ì„œë²„ ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ
    sys.argv = ["vllm_server.py"] + vllm_args
    vllm_api_server_main()

if __name__ == "__main__":
    main() 