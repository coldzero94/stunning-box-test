import argparse
import os
import torch
from typing import Optional, List, Dict, Any
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from peft import PeftModel
from threading import Lock, Thread
import time
import asyncio

# CUDA ë©”ëª¨ë¦¬ ì„¤ì • (ìµœì†Œí™”)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

class LLMChatHandler():
    def __init__(self, model_id: str, max_num_seqs: int, max_model_len: int, dtype: str):
        # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œì¸ì§€ í™•ì¸
        is_local_path = os.path.exists(model_id) and os.path.isdir(model_id)
        
        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (evaluate_translation.pyì™€ ë™ì¼í•˜ê²Œ)
        if is_local_path:
            print(f"ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            has_adapter = os.path.exists(os.path.join(model_id, "adapter_model.safetensors"))
            if has_adapter:
                print("ì–´ëŒ‘í„° ëª¨ë¸ ê°ì§€ë¨. ê¸°ë³¸ ëª¨ë¸ê³¼ ê²°í•©í•©ë‹ˆë‹¤.")
                base_model = "Qwen/Qwen2.5-14B-Instruct"
                print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘: {base_model}")
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    offload_buffers=True
                )
                print(f"ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {model_id}")
                self.model = PeftModel.from_pretrained(
                    self.model,
                    model_id,
                    offload_buffers=True
                )
            else:
                print("ì–´ëŒ‘í„° ëª¨ë¸ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.model = model_id
        else:
            print(f"Hugging Face ëª¨ë¸ ID ì‚¬ìš©: {model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = model_id

        self.lock = Lock()
        self.max_new_tokens = 1024
        self.temperature = 0.3
        self.top_p = 0.95
        self.top_k = 50
        self.repetition_penalty = 1.1

    def format_chat_history(self, history):
        """ì±„íŒ… ê¸°ë¡ì„ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        formatted = []
        for message in history:
            if isinstance(message, dict) and "role" in message and "content" in message:
                formatted.append(message)
        return formatted

    def generate_response(self, message, history):
        """ë™ê¸° í•¨ìˆ˜ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        
        # ëŒ€í™” ê¸°ë¡ í˜•ì‹í™”
        history_format = self.format_chat_history(history)
        
        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        history_format.append({"role": "user", "content": message})
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(history_format, tokenize=False, add_generation_prompt=True)
        
        with self.lock:
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                
                # ì…ë ¥ í† í° ìˆ˜ í™•ì¸
                input_token_length = inputs.input_ids.shape[1]
                if input_token_length > 3000:
                    yield "âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ë” ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    return
                
                # ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì • (í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°)
                streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
                
                # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
                gen_kwargs = {
                    "input_ids": inputs.input_ids,
                    "attention_mask": inputs.attention_mask,
                    "max_new_tokens": self.max_new_tokens,
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                    "top_k": self.top_k,
                    "repetition_penalty": self.repetition_penalty,
                    "do_sample": True,
                    "pad_token_id": self.tokenizer.pad_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                    "streamer": streamer
                }
                
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ì‹¤í–‰
                thread = Thread(target=self.model.generate, kwargs=gen_kwargs)
                thread.start()
                
                # ì‘ë‹µ ëˆ„ì 
                generated_text = ""
                is_first_token = True
                token_count = 0
                
                for new_text in streamer:
                    # ì²« í† í° ì‹œê°„ ê¸°ë¡ (ì§€ì—° ì‹œê°„ ê³„ì‚°ìš©)
                    if is_first_token:
                        first_token_time = time.time()
                        is_first_token = False
                    
                    generated_text += new_text
                    token_count += 1
                    
                    # ì§„í–‰ ì¤‘ì„ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œì‹œ
                    yield generated_text + " âŒ›"
                
                # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
                thread.join()
                
                # ìƒì„± ì™„ë£Œ ì‹œê°„ ê³„ì‚°
                end_time = time.time()
                generation_time = end_time - start_time
                first_token_latency = first_token_time - start_time if not is_first_token else 0
                
                # ìƒì„± ì†ë„ ê³„ì‚° (ì´ˆë‹¹ í† í°)
                if generation_time > 0:
                    tokens_per_second = token_count / generation_time
                else:
                    tokens_per_second = 0
                
                # ìƒì„± í†µê³„ ì¶”ê°€
                stats = f"\n\n---\nâœ… ìƒì„± ì™„ë£Œ (í† í°: {token_count}ê°œ, ì‹œê°„: {generation_time:.2f}ì´ˆ, ì†ë„: {tokens_per_second:.1f}í† í°/ì´ˆ, ì²« í† í°: {first_token_latency:.2f}ì´ˆ)"
                
                # ìµœì¢… í…ìŠ¤íŠ¸ ë°˜í™˜ (ì§„í–‰ ì¤‘ í‘œì‹œ ì œê±°í•˜ê³  í†µê³„ ì¶”ê°€)
                yield generated_text + stats
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del inputs
                torch.cuda.empty_cache()
                
            except Exception as e:
                yield f"Error: {str(e)}"

def main(args):
    print(f"Loading the model {args.model_id}...")
    hdlr = LLMChatHandler(model_id=args.model_id, max_num_seqs=args.max_num_seqs, max_model_len=args.max_model_len, dtype=args.dtype)

    with gr.Blocks(title=f"ğŸ¤— ìŠ¤í„°ë‹ ë°•ìŠ¤ ì±—ë´‡", fill_height=True) as demo:
        gr.Markdown(
            f"<h2>ë²ˆì—­ì„ ìœ„í•œ ì—ì´ì „íŠ¸</h2>"
        )
        
        # ìƒíƒœ í‘œì‹œ ì¶”ê°€
        status = gr.Markdown("âœ¨ ì¤€ë¹„ ì™„ë£Œ", elem_id="status")
        
        # ë©”ì‹œì§€ í˜•ì‹ ì‚¬ìš©ìœ¼ë¡œ ê²½ê³  ì œê±°
        chatbot = gr.Chatbot(type='messages', scale=20, render_markdown=True)
        
        with gr.Row():
            msg = gr.Textbox(
                show_label=False,
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                container=False,
                scale=8
            )
            submit = gr.Button("ì „ì†¡", scale=1)
        
        clear = gr.Button("ëŒ€í™” ì§€ìš°ê¸°")
        
        def user_message(message, history, status_text=None):
            """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
            if message.strip() == "":
                return "", history, "âœ¨ ì¤€ë¹„ ì™„ë£Œ"
            
            # ì´ë¯¸ historyê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if history is None:
                history = []
            
            # ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            history.append({"role": "user", "content": message})
            return "", history, "âŒ› ì‘ë‹µ ìƒì„± ì¤‘..."
        
        def bot_response(history, status_text=None):
            """ë´‡ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
            if not history:
                yield history, "âœ¨ ì¤€ë¹„ ì™„ë£Œ"
                return
            
            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
            last_user_message = history[-1]["content"]
            history_so_far = history[:-1]
            
            # ì‘ë‹µ ìƒì„±
            for new_text in hdlr.generate_response(last_user_message, history_so_far):
                new_history = history.copy()
                
                # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì¸ì§€ ì™„ë£Œëœê±´ì§€ í™•ì¸
                if new_text.endswith("âŒ›"):
                    status_update = "âŒ› ì‘ë‹µ ìƒì„± ì¤‘..."
                    new_text = new_text[:-1]  # ì§„í–‰ ì¤‘ í‘œì‹œ ì œê±°
                elif new_text.endswith("ì´ˆ)"):
                    status_update = "âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ"
                else:
                    status_update = "âœ¨ ì¤€ë¹„ ì™„ë£Œ"
                
                # ë´‡ ì‘ë‹µ ì¶”ê°€ (ë©”ì‹œì§€ í˜•ì‹)
                if len(new_history) > 0 and new_history[-1]["role"] == "assistant":
                    new_history[-1]["content"] = new_text
                else:
                    new_history.append({"role": "assistant", "content": new_text})
                
                yield new_history, status_update
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        msg.submit(
            user_message, 
            [msg, chatbot, status], 
            [msg, chatbot, status],
            queue=False
        ).then(
            bot_response,
            [chatbot, status],
            [chatbot, status]
        )
        
        submit.click(
            user_message, 
            [msg, chatbot, status], 
            [msg, chatbot, status],
            queue=False
        ).then(
            bot_response,
            [chatbot, status],
            [chatbot, status]
        )
        
        clear.click(lambda: ([], "âœ¨ ì¤€ë¹„ ì™„ë£Œ"), None, [chatbot, status], queue=False)

    demo.queue().launch(server_name="0.0.0.0", server_port=args.port)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="OSS Chatbot",
        description="Run open source LLMs from HuggingFace with a simple chat interface")

    parser.add_argument("--model-id", default="Qwen/Qwen2.5-14B-Instruct", help="HuggingFace model name for LLM.")
    parser.add_argument("--port", default=7860, type=int, help="Port number for the Gradio app.")
    parser.add_argument("--dtype", default="auto", type=str, help="Data type for model weights and activations.")
    parser.add_argument("--max-num-seqs", default=16, type=int, help="")
    parser.add_argument("--max-model-len", default=32767, type=int, help="")
    args = parser.parse_args()

    main(args) 