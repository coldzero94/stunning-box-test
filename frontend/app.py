import argparse
import os
import torch
from typing import Optional
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from vllm import SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.utils import random_uuid

class LLMChatHandler():
    def __init__(self, model_id: str, max_num_seqs: int, max_model_len: int, dtype: str):
        # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œì¸ì§€ í™•ì¸
        is_local_path = os.path.exists(model_id) and os.path.isdir(model_id)
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        if is_local_path:
            print(f"ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            
            # ì–´ëŒ‘í„° ëª¨ë¸ì¸ì§€ í™•ì¸
            has_adapter = os.path.exists(os.path.join(model_id, "adapter_model.safetensors"))
            if has_adapter:
                print("ì–´ëŒ‘í„° ëª¨ë¸ ê°ì§€ë¨. ê¸°ë³¸ ëª¨ë¸ê³¼ ê²°í•©í•©ë‹ˆë‹¤.")
                base_model = "Qwen/Qwen2.5-14B-Instruct"
                print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘: {base_model}")
                
                # CUDAë¥¼ ì‚¬ìš©í•˜ê³  GPU ë©”ëª¨ë¦¬ë¥¼ ì¤‘ê°„ ì´ìƒìœ¼ë¡œ ì‚¬ìš©
                print("CUDAë¥¼ ì‚¬ìš©í•˜ê³  GPU ë©”ëª¨ë¦¬ë¥¼ ì¤‘ê°„ ì´ìƒìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    trust_remote_code=True,
                    torch_dtype=torch.bfloat16,
                    device_map="cuda",  # CUDA ì‚¬ìš©
                    offload_folder=None,  # ì˜¤í”„ë¡œë“œ í´ë” ì‚¬ìš© ì•ˆ í•¨
                    offload_state_dict=False  # ì˜¤í”„ë¡œë“œ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ì•ˆ í•¨
                )
                
                # ì–´ëŒ‘í„° ë¡œë“œ ë° ì ìš©
                print(f"ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {model_id}")
                model.load_adapter(model_id)
                
                # ëª¨ë¸ ë³‘í•© í›„ ì‚¬ìš©
                print("ëª¨ë¸ ë³‘í•© ì¤‘...")
                # ë³‘í•©ëœ ëª¨ë¸ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
                merged_model_dir = model_id + "_merged"
                os.makedirs(merged_model_dir, exist_ok=True)
                
                # ëª¨ë¸ ë³‘í•© ë° ì €ì¥
                print("ëª¨ë¸ ë³‘í•© ë° ì €ì¥ ì¤‘...")
                # ë³‘í•©ëœ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ì „ì— ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ CPUë¡œ ë¡œë“œ
                for name, param in model.named_parameters():
                    if param.device.type == "meta":
                        print(f"ë©”íƒ€ í…ì„œ ê°ì§€: {name}")
                        # ë©”íƒ€ í…ì„œë¥¼ CPUë¡œ ë¡œë“œ
                        param.to("cpu")
                
                # ëª¨ë¸ ì €ì¥
                print("ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
                model.save_pretrained(merged_model_dir)
                
                # ëª¨ë¸ IDë¥¼ ë³‘í•©ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
                model_id = merged_model_dir
                print(f"ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ë¨: {model_id}")
            else:
                print("ì–´ëŒ‘í„° ëª¨ë¸ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print(f"Hugging Face ëª¨ë¸ ID ì‚¬ìš©: {model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            
        self.terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        def _guess_quantization(model_id) -> Optional[str]:
            if "awq" in model_id or "AWQ" in model_id:
                return "awq"
            if "bnb" in model_id:
                return "bitsandbytes"
            return "awq"  # ê¸°ë³¸ì ìœ¼ë¡œ AWQ ì–‘ìí™” ì‚¬ìš©

        def _guess_load_format(model_id) -> Optional[str]:
            if "bnb" in model_id:
                return "bitsandbytes"
            return "auto"

        # GPU ê°œìˆ˜ í™•ì¸
        num_gpus = torch.cuda.device_count()
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {num_gpus}")
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        if num_gpus > 0:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB ë‹¨ìœ„
            print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f} GB")
            
            # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ ë³‘ë ¬í™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            tensor_parallel_size = 1
            # GPU ë©”ëª¨ë¦¬ í™œìš©ë„ ë†’ì„
            gpu_memory_utilization = 0.95
            # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
            max_num_batched_tokens = 16384
        else:
            tensor_parallel_size = 1
            gpu_memory_utilization = 0.9
            max_num_batched_tokens = 4096
        
        engine_args = AsyncEngineArgs(
            model=model_id,
            task="generate",
            tokenizer=None,
            tokenizer_mode="auto",
            trust_remote_code=True,
            max_num_seqs=max_num_seqs,
            max_model_len=max_model_len,
            load_format=_guess_load_format(model_id=model_id),
            quantization=_guess_quantization(model_id=model_id),
            dtype=dtype,
            gpu_memory_utilization=gpu_memory_utilization,  # GPU ë©”ëª¨ë¦¬ í™œìš©ë„ ë†’ì„
            tensor_parallel_size=tensor_parallel_size,  # ë‹¨ì¼ GPU ì‚¬ìš©
            max_num_batched_tokens=max_num_batched_tokens,  # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
        )
        self.vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)

    async def chat_function(self, message, history):
        history.append({"role": "user", "content": message})
        prompt = self.tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)
        sampling_params = SamplingParams(
            stop_token_ids=self.terminators,
            max_tokens=1024,
            temperature=0.3,
            top_p=0.95,
            top_k=50,
            repetition_penalty=1.1)
        results_generator = self.vllm_engine.generate(prompt, sampling_params, random_uuid())
        async for request_output in results_generator:
            response_txt = ""
            for output in request_output.outputs:
                if output.text not in self.terminators:
                    response_txt += output.text
            yield response_txt

def main(args):
    print(f"Loading the model {args.model_id}...")
    hdlr = LLMChatHandler(model_id=args.model_id, max_num_seqs=args.max_num_seqs, max_model_len=args.max_model_len, dtype=args.dtype)

    with gr.Blocks(title=f"ğŸ¤— Chatbot with {args.model_id}", fill_height=True) as demo:
        with gr.Row():
            gr.Markdown(
                f"<h2>Chatbot with ğŸ¤— {args.model_id} ğŸ¤—</h2>"
                "<h3>Interact with LLM using chat interface!<br></h3>"
                f"<h3>Original model: <a href='https://huggingface.co/{args.model_id}' target='_blank'>{args.model_id}</a></h3>")
        cb = gr.Chatbot(type="messages", scale=20, render_markdown=False)
        gr.ChatInterface(hdlr.chat_function, chatbot=cb)

    demo.queue().launch(server_name="0.0.0.0", server_port=args.port)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="OSS Chatbot",
        description="Run open source LLMs from HuggingFace with a simple chat interface")

    parser.add_argument("--model-id", default="Qwen/Qwen2.5-14B-Instruct", help="HuggingFace model name for LLM.")
    parser.add_argument("--port", default=7860, type=int, help="Port number for the Gradio app.")
    parser.add_argument("--dtype", default="auto", type=str, help="Data type for model weights and activations.")
    parser.add_argument("--max-num-seqs", default=16, type=int, help="")
    parser.add_argument("--max-model-len", default=32767, type=int, help="")
    args = parser.parse_args()

    main(args) 