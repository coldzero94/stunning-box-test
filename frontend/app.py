import argparse
import os
import torch
from typing import Optional, List, Dict, Any
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from peft import PeftModel
from threading import Lock, Thread
import time

# CUDA ë©”ëª¨ë¦¬ ì„¤ì • (ìµœì†Œí™”)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

class LLMChatHandler():
    def __init__(self, model_id: str, max_num_seqs: int, max_model_len: int, dtype: str):
        # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œì¸ì§€ í™•ì¸
        is_local_path = os.path.exists(model_id) and os.path.isdir(model_id)
        
        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (evaluate_translation.pyì™€ ë™ì¼í•˜ê²Œ)
        if is_local_path:
            print(f"ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            has_adapter = os.path.exists(os.path.join(model_id, "adapter_model.safetensors"))
            if has_adapter:
                print("ì–´ëŒ‘í„° ëª¨ë¸ ê°ì§€ë¨. ê¸°ë³¸ ëª¨ë¸ê³¼ ê²°í•©í•©ë‹ˆë‹¤.")
                base_model = "Qwen/Qwen2.5-14B-Instruct"
                print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘: {base_model}")
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    offload_buffers=True
                )
                print(f"ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {model_id}")
                self.model = PeftModel.from_pretrained(
                    self.model,
                    model_id,
                    offload_buffers=True
                )
            else:
                print("ì–´ëŒ‘í„° ëª¨ë¸ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.model = model_id
        else:
            print(f"Hugging Face ëª¨ë¸ ID ì‚¬ìš©: {model_id}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = model_id

        self.lock = Lock()
        self.max_new_tokens = 1024
        self.temperature = 0.3
        self.top_p = 0.95
        self.top_k = 50
        self.repetition_penalty = 1.1

    async def chat_function(self, message, history):
        history_format = []
        for h in history:
            history_format.append({"role": "user", "content": h[0]})
            if h[1] is not None:
                history_format.append({"role": "assistant", "content": h[1]})
        
        history_format.append({"role": "user", "content": message})
        prompt = self.tokenizer.apply_chat_template(history_format, tokenize=False, add_generation_prompt=True)
        
        with self.lock:
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                
                # ì…ë ¥ í† í° ìˆ˜ í™•ì¸
                input_token_length = inputs.input_ids.shape[1]
                if input_token_length > 3000:
                    yield "âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ë” ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    return
                
                # ë‹µë³€ ìƒì„± ì¤‘ ë©”ì‹œì§€ ì‹¤ì‹œê°„ í‘œì‹œ
                yield "â³ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."
                
                # ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì • (í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°)
                streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
                
                # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
                gen_kwargs = {
                    "input_ids": inputs.input_ids,
                    "attention_mask": inputs.attention_mask,
                    "max_new_tokens": self.max_new_tokens,
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                    "top_k": self.top_k,
                    "repetition_penalty": self.repetition_penalty,
                    "do_sample": True,
                    "pad_token_id": self.tokenizer.pad_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                    "streamer": streamer
                }
                
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ì‹¤í–‰
                thread = Thread(target=self.model.generate, kwargs=gen_kwargs)
                thread.start()
                
                # ì²« ë²ˆì§¸ í† í° ëŒ€ê¸°
                first_token = True
                generated_text = ""
                
                # í† í° ìŠ¤íŠ¸ë¦¬ë°
                for new_text in streamer:
                    if first_token:
                        # ì²« ë²ˆì§¸ í† í°ì´ë©´ "ìƒì„± ì¤‘" ë©”ì‹œì§€ë¥¼ ëŒ€ì²´
                        generated_text = new_text
                        first_token = False
                    else:
                        # ì´í›„ í† í°ì€ ëˆ„ì 
                        generated_text += new_text
                    
                    # ê° í† í° ì¶œë ¥
                    yield generated_text
                
                # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
                thread.join()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del inputs
                torch.cuda.empty_cache()
                
            except Exception as e:
                yield f"Error: {str(e)}"

def main(args):
    print(f"Loading the model {args.model_id}...")
    hdlr = LLMChatHandler(model_id=args.model_id, max_num_seqs=args.max_num_seqs, max_model_len=args.max_model_len, dtype=args.dtype)

    with gr.Blocks(title=f"ğŸ¤— Chatbot with {args.model_id}", fill_height=True) as demo:
        with gr.Row():
            gr.Markdown(
                f"<h2>Chatbot with ğŸ¤— {args.model_id} ğŸ¤—</h2>"
                "<h3>Interact with LLM using chat interface!<br></h3>"
                f"<h3>Original model: <a href='https://huggingface.co/{args.model_id}' target='_blank'>{args.model_id}</a></h3>")
        
        chatbot = gr.Chatbot(scale=20, render_markdown=True)
        
        with gr.Row():
            with gr.Column(scale=8):
                msg = gr.Textbox(
                    show_label=False,
                    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                    container=False
                )
            with gr.Column(scale=1):
                submit = gr.Button("ì „ì†¡")
        
        clear = gr.Button("ëŒ€í™” ì§€ìš°ê¸°")
        
        def user(user_message, history):
            return "", history + [[user_message, None]]
        
        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            hdlr.chat_function, [chatbot[-1][0], chatbot], chatbot[-1][1]
        )
        
        submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            hdlr.chat_function, [chatbot[-1][0], chatbot], chatbot[-1][1]
        )
        
        clear.click(lambda: None, None, chatbot, queue=False)

    demo.queue().launch(server_name="0.0.0.0", server_port=args.port)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="OSS Chatbot",
        description="Run open source LLMs from HuggingFace with a simple chat interface")

    parser.add_argument("--model-id", default="Qwen/Qwen2.5-14B-Instruct", help="HuggingFace model name for LLM.")
    parser.add_argument("--port", default=7860, type=int, help="Port number for the Gradio app.")
    parser.add_argument("--dtype", default="auto", type=str, help="Data type for model weights and activations.")
    parser.add_argument("--max-num-seqs", default=16, type=int, help="")
    parser.add_argument("--max-model-len", default=32767, type=int, help="")
    args = parser.parse_args()

    main(args) 